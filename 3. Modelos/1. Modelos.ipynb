{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML\n",
    "\n",
    "### El aprendizaje automático (machine learning ML) es un método basado en algoritmos para analizar datos para buscar patrones y hacer predicciones precisas.\n",
    "\n",
    "### Los algoritmos de ML son entrenados de diferentes maneras y se pueden dividen en tres categorías.\n",
    "\n",
    "1. **No supervisados**\n",
    "2. **Supervisados**\n",
    "3. **Reforzamiento**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algoritmos de ML no supervisados\n",
    "\n",
    "ML no supervisado se basa en la idea de que una máquina puede aprender sin ninguna guía. Para el aprendizaje, utiliza datos que no están estructurados ni procesados. La limitación de estos algoritmos es que no tienen un punto de partida y por lo tanto, solo pueden realizar un número reducido de tareas. Las dos principales tareas que pueden realizar son:\n",
    "\n",
    "### a. Agrupamiento (clustering)\n",
    "\n",
    "Este algoritmo puede diferenciar dos cosas en función de sus características y separarlas en grupos (clusters). La agrupación en clústeres es excelente para resolver tareas como: filtrado de correo, detección de fraudes, agrupación jerárquica en clústeres para el análisis de documentos, etc.\n",
    "\n",
    "### a. Reducción de dimensionalidad\n",
    "\n",
    "Este tipo de algoritmo es el procesamiento y la simplificación de los datos al disminuir el número de características. El modelo de reducción de dimensionalidad reduce las características que no son esenciales para la tarea en cuestión, pero deja intactas la estructura y las características principales de los datos.\n",
    "\n",
    "La reducción de ruido y la visualización de datos son tareas comunes para los algoritmos de reducción de dimensionalidad. También se usa comúnmente como un paso intermedio en proyectos de ML más complejos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algoritmos de aprendizaje automático supervisados\n",
    "\n",
    "A diferencia del aprendizaje no supervisado, los algoritmos supervisados ​​requieren datos etiquetados, lo que le indicará al algoritmo cual es su significado. Esto significa que los modelos se entrenan en función de los datos que han sido procesados. \n",
    "\n",
    "El procesamiento de los datos es la supervisión que se realiza sobre el proceso de entrenamiento, de ahí el nombre de aprendizaje supervisado. Hay bastantes tipos de algoritmos los mas utilizados son: los de regresión, clasificación y pronóstico (forecasting).\n",
    "\n",
    "### a. Regresión\n",
    "\n",
    "Se requiera el análisis de valores continuos para encontrar una correlación entre diferentes variables. La regresión ayuda a buscar esta correlación y predecir una salida.\n",
    "\n",
    "Este tipo de algoritmo supervisado se usa comúnmente para predecir los precios o el valor de ciertos objetos en función de un conjunto de sus características. \n",
    "\n",
    "### b. Clasificación\n",
    "\n",
    "Similar a la agrupación no supervizada, la clasificación permite entrenar al algoritmo para agrupar diferentes objetos (valores) en categorías (o clases). La diferencia es que, ahora, el algoritmo sabe qué clase contiene qué objetos. \n",
    "\n",
    "A diferencia de la regresión, la clasificación se basa en un número limitado de valores. Puede ser binario  o multiclase .\n",
    "\n",
    "### c. Pronóstico\n",
    "\n",
    "Los algoritmos de pronóstico pueden \"predecir el futuro\", ya que pueden analizar los datos \"pasados y presentes\" en profundidad, buscar patrones ocultos y hacer predicciones basadas en este análisis.\n",
    "\n",
    "El análisis de tendencias es el fuerte de este tipo de algoritmo de aprendizaje automático. Es por eso que el pronóstico se usa comúnmente en negocios y finanzas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algoritmos de aprendizaje automático semisupervisados\n",
    "\n",
    "Hay casos en los que la combinación de los dos tipos de algoritmos puede brindarl mejores resultados, esto se debe a las características principales de cada tipo de algoritmo: el aprendizaje no supervisado aporta *simplicidad y eficiencia*, mientras que el aprendizaje supervisado tiene que ver con la *flexibilidad y los objetivos integrales*.\n",
    "\n",
    "Cuando se combinan dos tipos de diferentes algoritmos, se obtiene un aprendizaje **semisupervisado**. Este tipo de algoritmo ML le permite reducir significativamente el costo financiero, humano y de tiempo para anotar los datos. Al mismo tiempo, los algoritmos de aprendizaje semisupervisados ​​no están tan restringidos en la elección de tareas como los algoritmos de aprendizaje supervisado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El mejor algoritmo de aprendizaje automático\n",
    "\n",
    "1. Comprenda el objetivo de su proyecto\n",
    "\n",
    "Considerar el tipo de proyecto con el que está tratando: ¿qué tipo de salida necesita? ¿Necesitas un algoritmo de predicción basado en los datos anteriores? Recurra a los algoritmos de pronóstico supervisados. ¿Está buscando un modelo de reconocimiento de imágenes que funcione con fotos de baja calidad? La reducción de la dimensionalidad en combinación con la clasificación le ayudará con ello. ¿Necesitas enseñarle a tu modelo a jugar un juego nuevo? Un algoritmo de refuerzo será su mejor apuesta.\n",
    "\n",
    "2. Analizar los datos por tamaño, procesamiento y anotación requerida\n",
    "\n",
    "Qué entradas se tienen?. ¿Cómo son tus datos? ¿Es crudo, simplemente recolectado de donde sea y requiere procesamiento? ¿Es parcial, sucio y desestructurado? ¿O ya tiene un gran conjunto de datos anotados en sus manos? ¿Tiene suficientes datos o se requiere una recopilación adicional (o incluso una recopilación desde cero)? ¿Necesita dedicar tiempo a preparar sus datos para el proceso de capacitación o está listo para comenzar?. Los datos insuficientes, de baja calidad y sin procesar generalmente no se prestan a un gran entrenamiento de un algoritmo supervisado. Debe decidir si desea dedicar tiempo y recursos a preparar los mejores datos que pueda antes de comenzar el proceso de capacitación. De lo contrario, puede optar por algoritmos no supervisados, pero tenga en cuenta las limitaciones de dicha elección.\n",
    "\n",
    "3. Evalúar el Tiempo de Entrenamiento\n",
    "\n",
    "¿Lo necesita rápido incluso si eso significa una menor calidad de entrenamiento (y, respectivamente, predicciones)? Más datos y de mayor calidad conducen a una mejor capacitación. ¿Se puede asignar el tiempo necesario para la formación adecuada?\n",
    "\n",
    "4. Saber la linealidad de los datos\n",
    "\n",
    "Los algoritmos lineales (como la regresión lineal o las máquinas de vectores de soporte) son más simples y rápidos de entrenar. Sin embargo, no suelen usarse para problemas más complejos ya que tratan con datos lineales. Si los datos son multifacéticos, multidimensionales y tienen muchas correlaciones que se cruzan, es posible que los algoritmos lineales no sean suficientes para su tarea.\n",
    "\n",
    "5. Decida la cantidad de funciones y parámetros\n",
    "\n",
    "¿Qué tan complejo y preciso debe ser el modelo al final? ¿Se pueden especificar más características y parámetros para que el modelo los interprete y se entrene por más tiempo?. Darle a su algoritmo más tiempo para aprender puede ser una buena inversión en su futura precisión e interpretabilidad de salida."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para construir y usar un modelo\n",
    "\n",
    "1. Pre-procesameinto de los datos\n",
    "        \n",
    "        a. Importar los datos\n",
    "        b. Limpiar los datos\n",
    "        c. Separar en datos de entrenamiento y de prueba\n",
    "        d. Escalamiento de las caracteristicas\n",
    "\n",
    "        \n",
    "2. Modelado\n",
    "\n",
    "        a. Construir el modelo\n",
    "        b. Entrenar el modelo\n",
    "        c. Hacer prediciones con el modelo\n",
    "\n",
    "3. Evaluación\n",
    "\n",
    "        a. Calcular las métricas de rendimiento del modelo\n",
    "        b. Concluir si el modelo es eficiente o no\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importat datos\n",
    "\n",
    "https://www.kaggle.com/datasets/rakeshrau/social-network-ads\n",
    "\n",
    "Datos categóricos para determinar si un usuario compró un producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mainpath = \"../datasets/\"\n",
    "filename = 'Social_Network_Ads.csv'\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "dataset= pd.read_csv (fullpath)\n",
    "\n",
    "X = dataset.iloc[:, :-1].values # caracteristicas\n",
    "y = dataset.iloc[:, -1].values # variable a predecir\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separar datos en datos de entrenamiento y datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Escalamiento de las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/isadoji/Storage/github/ciencia-de-datos/3. Modelos/1. Modelos.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/isadoji/Storage/github/ciencia-de-datos/3.%20Modelos/1.%20Modelos.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m SVC \u001b[39m# maquinas de vector soporte\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/isadoji/Storage/github/ciencia-de-datos/3.%20Modelos/1.%20Modelos.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m classifier \u001b[39m=\u001b[39m SVC(kernel \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/isadoji/Storage/github/ciencia-de-datos/3.%20Modelos/1.%20Modelos.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/Storage/Software/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/svm/_base.py:201\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_targets(y)\n\u001b[1;32m    203\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    204\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    206\u001b[0m solver_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n",
      "File \u001b[0;32m~/Storage/Software/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/svm/_base.py:745\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_targets\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m    744\u001b[0m     y_ \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 745\u001b[0m     check_classification_targets(y)\n\u001b[1;32m    746\u001b[0m     \u001b[39mcls\u001b[39m, y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weight_ \u001b[39m=\u001b[39m compute_class_weight(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_weight, classes\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, y\u001b[39m=\u001b[39my_)\n",
      "File \u001b[0;32m~/Storage/Software/miniconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/multiclass.py:207\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    199\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[1;32m    201\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    202\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    206\u001b[0m ]:\n\u001b[0;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC # maquinas de vector soporte\n",
    "\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Obtener la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Validación del entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Visualizacion del resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                color= ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Kernel SVM (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Visualización del resultado de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                color = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Kernel SVM (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
