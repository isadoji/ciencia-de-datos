{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusión\n",
    "\n",
    "<center><div> <img src=\"fig/matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/></div><center>\n",
    "\n",
    "https://rapidminer.com/glossary/confusion-matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification # Generate a random n-class classification problem.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn import metrics\n",
    "\n",
    "# Generamos un dataset de dos clases\n",
    "x, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "x_train,x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state=2)\n",
    "# Entrenamos nuestro modelo de reg log\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix', size = 15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La clase final se determinará utilizando esta probabilidad y un umbral de decisión, este umbral puede ser ajustado para modificar el comportamiento de nuestro modelo para un problema específico.\n",
    "\n",
    "* Los dos tipos errores de predicción son:\n",
    "\n",
    "1. **Falso Positivo**: Predecir un evento cuando no hubo evento\n",
    "2. **Falso Negativo**: Predecir que no hubo un evento cuando sí que hubo evento\n",
    "\n",
    "El balanceo entre estos dos errores es lo que nos otorgará el nivel de umbral óptimo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva ROC Receiver Operating Characteristic (Característica Operativa del Receptor).\n",
    "\n",
    "Para interpretar correctamente las predicciones realizadas por modelos de clasificación binarios utilizaremos las curvas ROC y las curvas de precisión-sensibilidad (Precision-Recall).\n",
    "\n",
    "La **curva ROC** es una gráfica de la razón de falsos positivos (eje x) y de la razón de verdaderos positivos (eje y) dado un umbral, es decir, nos da la \"falsa alarma\" vs la tasa de éxito.\n",
    "\n",
    "La tasa de verdaderos positivos se calcula como el número de positivos verdaderos divididos entre el número de positivos verdaderos y de falsos negativos, es decir, la **sensibilidad** de nuestro modelo para prediciendo las clases positivas cuando la salida real es positiva. \n",
    "\n",
    "La tasa de falsos positivos se calcula como el número de falsos positivos dividido entre la suma de falsos positivos con los verdaderos negativos. Se considera como la tasa de \"falsa alarma\", es decir, que una clase negativa sea determinada por el modelo como positiva.\n",
    "\n",
    "La **especificidad** es la inversa de la tasa de falsos positivos. Se obtiene dividiendo el número total de verdaderos negativos entre la suma de los verdaderos negativos y los falsos positivos. \n",
    "\n",
    "La curva ROC permite comparar diferentes modelos para identificar cual otorga mejor rendimiento como clasificador y el área debajo de la curva (AUC) es el resumen de la calidad del modelo.\n",
    "\n",
    "* Valores pequeños en el eje X indican pocos falsos positivos y muchos verdaderos negativos\n",
    "* Valores grandes en el eje Y indican elevados verdaderos positivos y pocos falsos negativos\n",
    "\n",
    "1. El modelo morado representa a un clasificador perfecto. \n",
    "2. Las curvas más alejadas son los peores modelos. \n",
    "3. Un modelo aleatorio sin entrenar es una línea horizontal a media altura (0.5).\n",
    "\n",
    "<center><div> <img src=\"fig/rocv2.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div><center>\n",
    "\n",
    "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva de precisión-sensibilidad (Precision-Recall)\n",
    "\n",
    "La **precisión** se calcula como el número de verdaderos positivos entre la suma de verdaderos positivos y de falsos positivos. Describe cómo de bueno es el modelo a la hora de predecir las salidas de la clase positiva. \n",
    "\n",
    "La curva de precisión-sensibilidad enfrenta la precisión (eje y) con la sensibilidad (eje x) para diferentes umbrales.\n",
    "\n",
    "https://arxiv.org/pdf/1905.05441.pdf\n",
    "\n",
    "La curva de precisión-sensibilidad es útil cuando tenemos clases desbalanceadas, donde suele ser bastante común que haya muchos registros negativos (clase 0) y muy pocos positivos (clase 1), ya que no tiene en cuenta los falsos negativos. La curva de precisión-sensibilidad solo se preocupa de la clase positiva, es decir, de la clase minoritaria.\n",
    "\n",
    "<center><div> <img src=\"fig/PR.png\" alt=\"Drawing\" style=\"width: 400px;\"/></div><center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rendimiento del modelo se puede evaluar con dos valores:\n",
    "\n",
    "**ROC**-> Área bajo la curva (AUC)\n",
    "**PR** -> Valor F (F-Score) Calcula la media armónica de la precisión y la sensibilidad. \n",
    "\n",
    "\\begin{align}\n",
    "F_1 = 2* \\frac{P*R}{P+R}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curva ROC y AUC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Generamos un clasificador sin entrenar ,  0 a todo\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# Predecimos las probabilidades entrenando con lr\n",
    "lr_probs = lr.predict_proba(x_test)\n",
    "#Nos quedamos con las probabilidades de la clase positiva (la probabilidad de 1)\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# Calculamos el AUC\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "print('Sin entrenar: ROC AUC=%.3f' % (ns_auc))\n",
    "#print('Regresión Logística: ROC AUC=%.3f' % (lr_auc))\n",
    "# Calculamos las curvas ROC\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "\n",
    "# PR y F1\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, y_pred), auc(lr_recall, lr_precision)\n",
    "print('Regresión Logística: auc=%.3f f1=%.3f ' % (lr_auc, lr_f1))\n",
    "no_train = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Sin entrenar')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Regresión Logística')\n",
    "plt.xlabel('Falsos Positivos')\n",
    "plt.ylabel('Verdaderos Positivos')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot([0, 1], [no_train, no_train], linestyle='--', label='Sin entrenar')\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Regresión Logística')\n",
    "#Etiquetas de ejes\n",
    "plt.xlabel('Sensibilidad')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification # Generate a random n-class classification problem.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "###### Generamos un dataset de dos clases desbalanceadas\n",
    "x, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99,0.01], random_state=1)\n",
    "######\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state=2)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix', size = 15)\n",
    "\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "lr_probs = lr.predict_proba(x_test)\n",
    "lr_probs = lr_probs[:, 1]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "print('Sin entrenar: ROC AUC=%.3f' % (ns_auc))\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, y_pred), auc(lr_recall, lr_precision)\n",
    "print('Regresión Logística: auc=%.3f f1=%.3f ' % (lr_auc, lr_f1))\n",
    "no_train = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Sin entrenar')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Regresión Logística')\n",
    "plt.xlabel('Falsos Positivos')\n",
    "plt.ylabel('Verdaderos Positivos')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot([0, 1], [no_train, no_train], linestyle='--', label='Sin entrenar')\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Regresión Logística')\n",
    "#Etiquetas de ejes\n",
    "plt.xlabel('Sensibilidad')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
