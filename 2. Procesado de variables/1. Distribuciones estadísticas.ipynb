{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribución estadística \n",
    "\n",
    "Algunos modelos de machine learning requieren que la variable a predecir (objetivo) se comporte estadísticamente de una manera determinada, por ejemplo: \n",
    "\n",
    "* LM-> distribución normal\n",
    "\n",
    "* GLM-> distribución exponencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from fitter import Fitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de probabilidad\n",
    "\n",
    "1. Un experimento estadístico es cualquier proceso que proporciona datos. \n",
    "2. Estos datos tienen que convertirse en descripciones numéricas del resultado.\n",
    "3. Estas descripciones numéricas son observaciones aleatorias. \n",
    "4. A las observaciones aleatorias se les considera como la expresión en cada caso concreto de una variable aleatoria que toma valores en los resultados del experimento.\n",
    "\n",
    "**Variable aleatoria** es una variable matemática cuyos posibles valores son las descripciones numéricas de todos los resultados posibles de un experimento\n",
    "estadístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #float [0,1]\n",
    "for i in range(5):\n",
    "    print(random.random())\n",
    "    print(random.randrange(1, 100, 2))#int [a,b] step \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden distinguir distintos tipos de variables aleatorias:\n",
    "\n",
    "1. _Variables cuantitativas_: son las que resultan de experimentos cuyos resultados son directamente numéricos.\n",
    "\n",
    "2. _Variables cualitativas_: son las que proceden de experimentos cuyos resultados expresan una cualidad no numérica que necesita ser cuantificada.\n",
    "\n",
    "3. _Variables discretas_: son aquellas que se define sobre un espacio muestral numerable, finito o infinito. \n",
    "\n",
    "4. _Variables continuas_: son aquellas que se definen sobre un espacio asimilable al conjunto de los números reales, es decir, un espacio no numerable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables aleatorias discretas\n",
    "\n",
    "Una variable aleatoria discreta toma cada uno de sus valores con una determinada probabilidad.\n",
    "\n",
    "**Función de probabilidad**: es una función tal que, al sustituir x por un valor de la variable, el valor que toma la función es la probabilidad de que la variable X asuma el valor x.\n",
    "\n",
    "La función de probabilidad se representa como: \n",
    "\n",
    "$f(x)=P(X=x)$\n",
    "\n",
    "Las funciones de probabilidad sólo se definen para los valores de la variable aleatoria y deben cumplir tres propiedades:\n",
    "\n",
    "1. $\\forall  x \\in R, f(x) \\geq 0$\n",
    "2. $\\sum_{x} f(x) = 1$\n",
    "3. $P(X=x)=f(x)$\n",
    "\n",
    "**Función de distribución** La función de distribución se define para todos los números reales, no sólo para los valores de la variable. Su máximo es siempre 1 pues cuando el valor que se sustituye es mayor o igual que el valor máximo de la variable, la probabilidad de que ésta tome valores menores o iguales que el sustituido es la probabilidad del espacio muestral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables aleatorias continuas\n",
    "\n",
    "**Función de densidad** Una variable aleatoria continua tiene la característica de tomar cada uno de sus valores con probabilidad infinitesimal. \n",
    "\n",
    "Puede calcularse la probabilidad de que la variable tome valores en determinados intervalos.\n",
    "\n",
    "$P(a \\leq X \\leq b) = P(X = a) + P(a < X < b) + P(X = b) = P(a < X < b)$\n",
    "\n",
    "La función de densidad debe cumplir tres condiciones análogas a las de la función de probabilidad:\n",
    "\n",
    "1. $\\forall  x \\in R f(x) \\geq  0$\n",
    "2. $\\int_{-\\infty}^{\\infty} f(x) dx = 1$\n",
    "3. $P(a \\leq X \\leq b)=\\int_{a}^{b}f(x) dx$\n",
    "\n",
    "**Función de densidad** es una función continua tal que su integral entre los extremos de un intervalo nos da el valor de la probabilidad de que X tome valores en ese intervalo.\n",
    "\n",
    "$P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando Normal\n",
    "mu, sigma = 10, 1.2\n",
    "x = np.linspace(st.norm(mu, sigma).ppf(0.01),\n",
    "                  st.norm(mu, sigma).ppf(0.99), 100)\n",
    "normal = st.norm(mu, sigma).pdf(x) # FDP\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(x, normal, label='normal')\n",
    "plt.title('Función de Densidad de Probabilidad')\n",
    "plt.ylabel('P(x)')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución conjunta de dos variables\n",
    "\n",
    "Cuando tenemos dos variables aleatorias X y Y, si queremos estudiarlas conjuntamente debemos establecer una relación que ligue los valores de una con los de la otra.\n",
    "\n",
    "Para variables discretas, se puede establecer una función de probabilidad para las posibles parejas de valores de ambas variables; a esta función se le llama **función de probabilidad conjunta, f(x,y).**\n",
    "\n",
    "$P[(X=x)\\cap(Y=y)]= f(x,y)$\n",
    "\n",
    "Que puede ser discreta o continua y que cumple las condiciones de la función de probabilida o función de densiadad\n",
    "\n",
    "\n",
    "1. $\\forall  x,y \\in R f(x,y) \\geq  0$\n",
    "2. $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1$\n",
    "3. $P[(x,y)\\in A]=\\int\\int_A f(x,y) dx dy$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias librerías en python que permiten identificar a qué distribución se ajustan mejor los datos, una de ellas es fitter. Esta librería permite ajustar cualquiera de las 80 distribuciones implementadas en scipy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valor esperado de una variable\n",
    "\n",
    "**Variables aleatorias independientes**: dos variables aleatorias X y Y, discretas o continuas cuyas funciones de probabilidad o densidad son $g(x)$ y $h(y)$, respectivamente, con función de probabilidad o densidad conjunta $f(x,y)$, son estadísticamente independientes si y sólo si $f(x,y)=g(x)\\cdot f(x)$\n",
    "\n",
    "Supongamos que hemos realizado n veces un experimento aleatorio que genera una variable X. El valor medio del experimento en estas n repeticiones es la suma de los productos de los valores de la variable por su frecuencia relativa. Cuando n sea igual a infinito, el valor medio del experimento se llama valor esperado o esperanza matemática:\n",
    "\n",
    "$\\mathbb{E}[X]=x_1p(X=x_1)+\\ldots+x_ip(X=x_i)=\\sum_i^nx_ip(x_i)$\n",
    "\n",
    "Para una variable aleatoria continua, la esperanza se calcula mediante la integral de todos los valores y la función de densidad:\n",
    "\n",
    "$\\mathbb{E}[X]=\\int_{-\\infty}^\\infty x f(x) dx$.\n",
    "\n",
    "La esperanza también se suele simbolizar con $\\mathbb{E}[X]=\\mu$\n",
    "\n",
    "**Propiedades**\n",
    "\n",
    "1. Si X es siempre positiva, entonces siempre lo es $\\mathbb{E}[X]$.\n",
    "2. La esperanza matemática de una constante es igual a esa misma constante, es decir, $\\mathbb {E}[c]=c$.\n",
    "3. Si X está delimitada por dos números reales, a y b, tal que: $a \\leq X \\leq b$, entonces también lo está su media: $a \\leq \\mathbb{E}[X] \\leq b$\n",
    "4. Linealidad. Si existe $\\mathbb{E}[X]$ y se considera $Y=a+bX$, entonces $\\mathbb{E}[Y]=\\mathbb{E}[a+bX]=a+b\\mathbb{E}[X]$\n",
    "\n",
    "además la esperanza es un operador **lineal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# Create grid and multivariate normal\n",
    "x = np.linspace(st.norm(mu, sigma).ppf(0.01),\n",
    "                  st.norm(mu, sigma).ppf(0.99), 100)\n",
    "\n",
    "x = np.linspace(-10,10,500)\n",
    "y = np.linspace(-10,10,500)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X \n",
    "pos[:, :, 1] = Y\n",
    "# Create a frozen RV object\n",
    "mean = np.array([1, 2])\n",
    "cov  = np.array([[3,0],[0,15]])\n",
    "rv = multivariate_normal(mean,cov)\n",
    "# Make a 3D plot\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('P(x, y)')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentos de una variable\n",
    "\n",
    "Dada una variable aleatoria X con función de probabilidad o densidad f(x) podemos definir una función de X que sea igual a la diferencia entre la variable y su media elevada a un exponente entero no negativo:\n",
    "\n",
    "$z(x)=(x-\\mu)^k$ siendo  $k\\in Z, k\\geq 0$\n",
    "\n",
    "El valor esperado de $z(x)$ es el k-ésimo momento de la variable X respecto a su origen:\n",
    "\n",
    "$\\mu_2=\\mathbb{E}[(x-\\mu )^2]= \\mathbb{E}[X^{2}]-\\mu^{2}=\\mathbb{E}[X^{2}]-\\mathbb{E}[X]^{2}= \n",
    "\\left \\{ \\begin{matrix} \\sum_x (x-\\mu)^2 f(x) & si~X~es~discreta \\\\ \n",
    "\\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) dx & si~X~es~continua \\end{matrix}\\right.$\n",
    "\n",
    "$k=0, \\mu_0=1$;\n",
    "\n",
    "$k=1, \\mu_1=\\mathbb{E}[(x-\\mu)^1]=\n",
    "\\mathbb{E}[(x-\\mu)]=\\mathbb{E}[X]-\\mu=0$\n",
    "\n",
    "$k=2, \\mu_2=\\mathbb{E}[(x-\\mu )^2]=\\sigma^2$ \n",
    "\n",
    "### Varianza\n",
    "\n",
    "La varianza de una variable mide la dispersión de sus valores respecto al valor central $\\mu$\n",
    "\n",
    "$\\mu_2=\\mathbb{E}[(x-\\mu )^2]= \\mathbb{E}[X^{2}]-\\mu^{2}=\\mathbb{E}[X^{2}]-\\mathbb{E}[X]^{2}= \n",
    "\\left \\{ \\begin{matrix} \\sum_x (x-\\mu)^2 f(x) & si~ X~ es~ discreta \\\\ \n",
    "\\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) dx & si~ X~  es~ continua \\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "Es decir, la varianza de una variable es igual a la media de los cuadrados menos el cuadrado\n",
    "de la media.\n",
    "\n",
    "La varianza se expresa en unidades cuadráticas que no siempre tienen una interpretación clara. \n",
    "\n",
    "**Desviación estandar $\\sigma$:** Medida de la\n",
    "dispersión que que se calcula como la raíz cuadrada positiva de la varianza. La desviación estandar se mide en las mismas unidades que la\n",
    "variable\n",
    "\n",
    "$\\sigma_x =+\\sqrt{\\sigma^2_x}$\n",
    "\n",
    "La desviación estandar no resuelve todos los problemas que se pueden plantear, por ejemplo la comparación de situaciones en las que la unidad de medida sea diferente. Para resolver esta cuestión se define una medida adimensional de\n",
    "la variabilidad que es el coeficiente de variación, $C_V$, que se calcula como el cociente entre la desviación típica y la media.\n",
    "\n",
    "$C_V=\\frac{\\sigma}{\\mu}$\n",
    "\n",
    "o porcentual\n",
    "\n",
    "$C_V=100 \\cdot\\frac{\\sigma}{\\mu}$\n",
    "\n",
    "### Varianza de variables asociadas\n",
    "\n",
    "Supongamos que tenemos dos variables aleatorias X y Y, discretas o continuas, con función\n",
    "de probabilidad o densidad conjunta f(x,y) y definimos una función z(x,y):\n",
    "\n",
    "$z(x,y)=(x-\\mu_x)(y-\\mu_y)$\n",
    "\n",
    "Al valor esperado de z(x,y) se le llama **covarianza $\\sigma_{xy}$** o **cov(x,y)** de las variables X y Y.\n",
    "\n",
    "$\\mu_{xy}=\\mathbb{E}[(x-\\mu_{x})][(y-\\mu_{y})]=$\n",
    "$\\left\\{\\begin{matrix} \n",
    "\\sum_x\\sum_y (x-\\mu_x)(y-\\mu_y) f(x,y) &\n",
    "si~ X~ y~ Y~ son~ discreta~ \\\\ \n",
    "\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} (x-\\mu_x)(y-\\mu_y) f(x,y) dx & \n",
    "si~ X~ y~ Y~ son~ continua \\end{matrix}\\right.$\n",
    "\n",
    "La covarianza es una medida de la variación común a dos variables y, por tanto, una medida\n",
    "del grado y tipo de su relación.\n",
    "1. $\\sigma_{xy}$ es positiva si los valores más grandes de X están asociados a los valores más grandes de Y y viceversa.\n",
    "2. $\\sigma_{xy}$ es negativa si los más bajos de X están asociados a los valores más bajos de Y y\n",
    "viceversa.\n",
    "3. Si X y Y son variables aleatorias independientes cov(x,y) = 0 (la independencia es condición suficiente pero no necesaria para que la cov(x,y) sea nula.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <img src=\"fig/covarianza.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La covarianza de dos variables se puede calcular como:\n",
    "\n",
    "$\\sigma_{xy}=\\mathbb{E}[(x-\\mu_x)(y-\\mu_y)]=\\mathbb{E}[X\\cdot Y]-\\mu_x \\mu_y=\\mathbb{E}[X\\cdot Y]-\\mathbb{E}[X]\\cdot\\mathbb{E}[Y]$\n",
    "\n",
    "La covarianza se expresa en términos del producto de las unidades de medida de\n",
    "ambas variables, lo cual no siempre es fácilmente interpretable. Por otra parte también es difícil comparar situaciones diferentes entre sí. Ambos problemas se solucionan mediante la definición del coeficiente de correlación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coeficiente de correlación $\\rho$:** Es el cociente entre la covarianza y el producto de las desviaciones estandar de las dos variables.\n",
    "\n",
    "$\\rho=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$\n",
    "\n",
    "1. La correlación toma valores entre -1 y 1, siendo su signo igual al de la covarianza.\n",
    "2. Correlaciones con valor absoluto 1 implican que existe una asociación matemática lineal perfecta, positiva o negativa, entre las dos variables.\n",
    "3. Correlaciones iguales a 0 implican ausencia de asociación (las variables independientes tienen correlación 0, pero la independencia es condición suficiente pero no necesaria.)\n",
    "4. Correlaciones con valores absolutos intermedios indican cierto grado de asociación entre los valores de las variables.\n",
    "\n",
    "La covarianza se expresa en términos del producto de las unidades de medida de\n",
    "ambas variables, lo cual no siempre es fácilmente interpretable. Por otra parte también es difícil comparar situaciones diferentes entre sí. Ambos problemas se solucionan mediante la definición del coeficiente de correlación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asimetría\n",
    "\n",
    "El tercer momento respecto de la media, mide la asimetría de la distribución, es decir, si\n",
    "existen o no observaciones muy extremas en algún sentido con frecuencias razonablemente altas.\n",
    "\n",
    "$k=3, \\mu_3=\\mathbb{E}[(x-\\mu)^3]$ \n",
    "\n",
    "\n",
    "1. Si la asimetría es negativa, la variable toma valores a la izquierda de la media que  a la derecha. \n",
    "\n",
    "2. Si la asimetría es positiva, la variable toma valores a la derecha de la media que a la izquierda. \n",
    "\n",
    "3. Si la asimetría es cero, la districución tiene los mismos valores a la derecha y a la izquierda de la media (por ejemplo la distribución normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <img src=\"fig/simetria.png\" alt=\"Drawing\" style=\"width: 800px;\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La asimetría tiene el mismo problema que la varianza y la covarianza en cuanto a sus\n",
    "unidades de medida y, por ello, normalmente se utiliza una medida adimensional de la asimetría\n",
    "que es el coeficiente de asimetría de Fisher , $g_1$ , que se calcula como el cociente entre el tercer momento y el cubo de la desviación estandar.\n",
    "\n",
    "$g_1=\\frac{\\mu_3}{\\sigma_3}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curtosis\n",
    "\n",
    "El cuarto momento respecto de la media, mide la forma de la distribución de probabilidad o **curtosis**.\n",
    "\n",
    "$k=4, \\mu_4=\\mathbb{E}[(x-\\mu)^4]$ \n",
    "\n",
    "1. Curtosis pequeña (platicúrticas): curvas o histogramas con colas cortas y aspecto aplanado o en meseta.\n",
    "\n",
    "2. Curtosis grande (leptocúrtica): gráfica alta y estilizada, con colas largas y pesadas.\n",
    "\n",
    "La curtosis de una variable siempre es positiva y se mide en la unidades de la variable\n",
    "elevadas a potencia 4. Por tanto, tiene los mismos problemas relacionados con las\n",
    "unidades de medida y las escalas que los momentos menores.\n",
    "\n",
    "**Coefeiciente de curtosis** $g_2$: Medida adimensional de la curtosis, se calcula como el\n",
    "cociente entre el cuarto momento y el cuadrado de la varianza, al que se le resta 3 unidades. Esta corrección se debe a que, sin ella, las variables normales tendrían coeficiente de curtosis igual a 3; al restar 3 conseguimos que el coeficiente de curtosis de la variable normal sea 0 y que las variables platicúrticas tengan coeficiente de curtosis negativo y la leptocúrticas positivo.\n",
    "\n",
    "$g_2=\\frac{\\mu_4}{\\sigma_4}-3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = np.random.normal(0, 0.5, 10000)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(gauss, 50)  \n",
    "plt.show()\n",
    "print(\"Estadistica:\", \"media\", np.mean(gauss), \"varianza\", np.var(gauss) , \"asimetría\", st.skew(gauss),\"curtosis\", st.kurtosis(gauss)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones teóricas de densidas de probabilidad\n",
    "\n",
    "1. Variables aleatorias x: números reales, r: enteros\n",
    "2. Parámetros: $\\theta$\n",
    "3. Función de densidad de probabilidad: $f(x;\\theta)$ o $f(x|\\theta)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribucion gaussiana o normal\n",
    "\n",
    "$f(x;\\mu,\\sigma) = \\frac{1}{{\\sigma\\sqrt{2\\pi}}}e^{-\\frac{\\left(x-\\mu\\right) ^2}{2\\sigma^2}}$\n",
    "\n",
    "1. Dos parámetros, $\\mu$: valor medio, $\\sigma$: desviación estándar\n",
    "\n",
    "### Producto de dos gaussianas:\n",
    "\n",
    "$f(x,y;\\mu_x, mu_y,\\sigma_x, \\sigma_y) = \\frac{1}{{\\sigma_x \\sigma_y 2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{(x-\\mu_x)^2}{\\sigma_x^2}+\\frac{(y-\\mu_y)^2}{\\sigma_y^2}\\right)}$\n",
    "\n",
    "1. Correlación $\\rho=\\frac{cov[x,y]}{\\sigma_x \\sigma_y}$, pero la matriz de covarianza se puede escibir como $E[x,y]-E[x]E[y]=V_{x,y}$, entonces $\\rho=\\frac{V_{x,y}}{\\sigma_x\\sigma_y}$ y \n",
    "\n",
    "$f(x;\\mu,V)=\\frac{1}{(2\\pi)^{N/2}|V|}e^{-\\frac{1}{2}\\left((x-\\mu)^T V^{-1}(x-\\mu)\\right)}$\n",
    "\n",
    "### Función de error\n",
    "\n",
    "Probabilidad de que una variable aleatoria caiga dentro del rango de $1\\sigma$\n",
    "\n",
    "$erf(\\gamma)=\\frac{2}{\\sqrt{\\pi}}\\int_0^\\gamma e^{-x^2}dx=1= 68\\%$\n",
    "\n",
    "Para considerar un nuevo descubrimineto como correcto tiene que estar dento de $5\\sigma$ que es un p-valor=$5.7 x10^{-7}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función gaussiana\n",
    "from scipy.stats import norm\n",
    "\n",
    "norm_rnd = norm.rvs(size=1000)\n",
    "mu, sigma = 0, 1.2 # media y desviación estandar\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(norm_rnd, 20)\n",
    "plt.title('Distribución gaussiana')\n",
    "plt.ylabel('$f~(x;\\mu,\\sigma)$ ')\n",
    "plt.xlabel('x')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "gausspdf = st.norm(mu, sigma)\n",
    "x = np.linspace(gausspdf.ppf(0.01),\n",
    "                gausspdf.ppf(0.99), 100)\n",
    "gp = gausspdf.pdf(x) # Función de Probabilidad\n",
    "plt.plot(x, gp)\n",
    "plt.title('Distribución de probabilidad gaussiana')\n",
    "plt.ylabel('$P~(x;\\mu,\\sigma)$ ')\n",
    "plt.xlabel('x')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de Poisson\n",
    "\n",
    "$f(n;\\nu)=\\frac{\\nu^n}{n!}e^{-\\nu}$ \n",
    "\n",
    "1. $n$ discreto; $\\nu$ continua\n",
    "2. Describe la probabilidad de que $n$ eventos sucedan, cuando la probabilidad de expectación es $\\nu$\n",
    "3. Su desviación estándar es $\\sigma=\\sqrt{\\nu}$\n",
    "4. Funciona mejor mientras mas grande sea $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Poisson\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "nu = 4\n",
    "poisson_rnd = poisson.rvs(nu, 0, 10000)\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(poisson_rnd, 30)\n",
    "plt.title('Distribución de Poisson')\n",
    "plt.ylabel('$f~(x;\\mu,\\sigma)$ ')\n",
    "plt.xlabel('x')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "poissonpdf = poisson(nu)\n",
    "x = np.arange(poissonpdf.ppf(0.01),\n",
    "              poissonpdf.ppf(0.99))\n",
    "pp = poisson.pmf(x,nu) # Función de Probabilidad\n",
    "plt.plot(x, pp)\n",
    "plt.title('Distribución de probabilidad de Poisson')\n",
    "plt.ylabel('$P~(x;nu ) $ ')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución binomial\n",
    "$f(r;N,p) = \\frac{N!}{r!(N-r)!}p^r(1-p)^{n-r}$\n",
    "\n",
    "1. $\\binom{N}{r}=\\frac{N!}{r!(N-r)!}$ combinaciones de N en r; $q=1-p$\n",
    "2. Probabilidad de tener $r$ casos acertados de $N$ eventos, dada una probabilidad intrínseca de $p$\n",
    "3. Desviación estrndar $\\sigma=\\sqrt{Npq}$\n",
    "4. Cuando $p$ es muy grande, se aproxima a la distribución de Poisson\n",
    "5. Cuando $p$ es muy grande y $N$ también es muy grande, se aproxima a una distribución gaussiana\n",
    "6. Si existen varias proabilidades intrínsecas $p_n={p_1,p_2,...p_n}$ entonce la distibución binomial es:\n",
    "\n",
    "$f(r_i;N,p_i)=\\frac{N!}{\\prod r_i!}\\prod p_i^{r_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función Binomial\n",
    "from scipy.stats import binom\n",
    "n, p = 50, 0.8\n",
    "binom_rnd = binom.rvs(n, p, size=1000)\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "binomial = plt.hist(binom_rnd, 30)\n",
    "plt.title('Distribución de Poisson')\n",
    "plt.ylabel('$f~(r; N, P)$ ')\n",
    "plt.xlabel('r')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "binompdf = binom(n, p)\n",
    "x = np.arange(binom.ppf(0.01,n,p),\n",
    "              binom.ppf(0.99,n,p))\n",
    "bp = binompdf.pmf(x) # Función de Probabilidad\n",
    "plt.plot(x, bp)\n",
    "plt.title('Distribución de probabilidad de Poisson')\n",
    "plt.ylabel('$P~(r; N, P ) $ ')\n",
    "plt.xlabel('r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Distribución de Breit-Wigner (Cauchy)\n",
    "\n",
    "$f(E;M,\\Gamma) =\\frac{1}{2\\pi} \\frac{\\Gamma}{(E-M)^2+(\\Gamma/2)^2}$\n",
    "\n",
    "1. Variación de la energía de la sección eficaz de producida en la formación de un estado con masa $M$ y anchura $\\Gamma$ \n",
    "2. Si $x=\\frac{E-M}{\\Gamma/2}$ entonces $f(x)=\\frac{1}{\\pi}\\frac{1}{1+x^2}$ (Cauchy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de Landau\n",
    "$f(\\lambda)=\\frac{1}{\\pi}\\int_0^\\infty e^{-u ln u -\\lambda u} \\sin(\\pi u)du$\n",
    "\n",
    "1. Distribución de probabilidad de la pérdida de energía de una partícula cargada en presencia de un campo electromagnético.\n",
    "2. $\\Delta$ es la energía pérdida a partir de una energía inicial $\\Delta_0$\n",
    "3. El pico de la distribución se encuentra en $\\lambda=\\Delta-\\Delta_0/\\xi$, donde $\\xi$ es una escala que depende del material."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución binomial negativa\n",
    "$f(r;k,p) = \\frac{(k+r-1)!}{r!(k-1)!}q^k p^r$\n",
    "\n",
    "1. Probabilidad de que $r$ casos acertados en $N$ permutaciones, antes de tener $k$ casos erroneos\n",
    "2. Permutaciones: $\\binom{-k}{r}=(-1)\\frac{(k+r-1)!}{r!(k-1)!}$\n",
    "3. Casoos erroneos $q=1-p$\n",
    "4. Valor medio $\\nu=\\frac{p}{q}k$ y la varianza $V=\\frac{p}{q^2}$\n",
    "5. Para $p$ muy grande, $k$ muy pequeña y $\\mu=pk$ la distribución binomial negativa se aproxima a una distribución de Poisson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de Student\n",
    "\n",
    "$f(t;n)=\\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(\\frac{n}{2})}(1+\\frac{t^2}{2})^{\\frac{n+1}{2}}$\n",
    "\n",
    "1. $t=\\frac{x-\\mu}{\\sigma}$ \n",
    "2. Para n muy grande la distribución de Studen se aproxima a una distribución gaussiana\n",
    "3. Para n muy chicas, el ancho de la districución aumenta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución $\\chi^2$\n",
    "\n",
    "La desviación cuadrática media de una función $g(x)$ y $n$ mediciones de ${(x_i,y_i)}$ es:\n",
    "\n",
    "$\\chi^2=\\sum_{i=1}^{n}\\left(\\frac{\\gamma_i - g(x_i)}{\\sigma_i}\\right)^2$\n",
    "\n",
    "donde $\\sigma_i$ es el error gaussiano de la i-ésima medición.\n",
    "\n",
    "Cada término contribuye al error, por lo que $E[f(\\chi^2;n)]=n$ donde $\\sigma=\\sqrt{2n}$ y $f(\\chi^2;n)$ es la distribución $\\chi^2$:\n",
    "\n",
    "$f(\\chi^2;n)=\\frac{2^{-n/2}}{\\Gamma(n/2)}\\chi^{n-2}e^{-\\chi^2/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución $\\chi^2$ es una prueba estadística (Pearson) que se usa para determinar si existe una diferencia estadísticamente significativa entre la frecuencia esperada y la frecuencia observada de las variables categóricas. Es una distribución continua que se especifica por los grados de libertad y el parámetro de no centralidad. La distribución es positivamente asimétrica, pero la asimetría disminuye al aumentar los grados de libertad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <img src=\"fig/distributions.png\" alt=\"Drawing\" style=\"width: 1000px;\"/></div>\n",
    "\n",
    "https://www.indiumsoftware.com/blog/statistical-distributions/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitter: Esta librería permite ajustar distribuciones a los datos.\n",
    "\n",
    "https://pypi.org/project/fitter/0.2.0/\n",
    "\n",
    "$ pip install fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"../datasets/\"\n",
    "filename = \"titanic-kaggle/train.csv\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "data = pd.read_csv (fullpath)\n",
    "data1 = data\n",
    "data1 = data1.drop(['Cabin','Name','Ticket'], axis=1)\n",
    "data1 = data1[data1['Embarked'].notna()]\n",
    "data1['Age'] = data1['Age'].fillna(-1)\n",
    "\n",
    "### Datos ordinales\n",
    "\n",
    "data1['Embarked'] = data1['Embarked'].map({'Q': 2, 'C': 1, 'S': 0})\n",
    "data1['Sex'] = data1['Sex'].map({'female': 1, 'male': 0})\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 distribuciones\n",
    "distribuciones = ['cauchy', 'chi2', 'expon',  'exponpow', 'gamma',\n",
    "                  'norm', 'powerlaw', 'beta', 'logistic']\n",
    "# Atributo\n",
    "fitter = Fitter(data1.Age, distributions=distribuciones)\n",
    "fitter.fit()\n",
    "fitter.summary(Nbest=10, plot=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sumsquare_error: Akaike information criterion (aic) $\\rightarrow 2k-ln(L)$. *k* es el número de parámetros en el modelo estadístico, y *L* es el máximo valor de la función de verosimilitud para el modelo estimado.\n",
    "\n",
    "https://es.wikipedia.org/wiki/Criterio_de_informaci%C3%B3n_de_Akaike\n",
    "\n",
    "* bayesian information criterion (bic) $\\rightarrow kln(n)-2ln(L)$ \n",
    "\n",
    "https://es.wikipedia.org/wiki/Criterio_de_informaci%C3%B3n_bayesiano\n",
    "\n",
    "* ks_statistic: Kolmogorov–Smirnovit. Compara las dos distribuciones acumulativas y devuelve la diferencia máxima entre ellas.\n",
    "\n",
    "https://es.wikipedia.org/wiki/Prueba_de_Kolmog%C3%B3rov-Smirnov\n",
    "\n",
    "Por defecto, la función clasifica las cinco mejores distribuciones según los valores de **sumsquare_error** en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.get_best(method = 'sumsquare_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
    "axes = axes.flat\n",
    "for i, colum in enumerate(data1):\n",
    "    sns.histplot(\n",
    "        data     = data1,\n",
    "        x        = colum,\n",
    "        stat     = \"count\",\n",
    "        kde      = True,\n",
    "        color    = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\n",
    "        line_kws = {'linewidth': 2},\n",
    "        alpha    = 0.3,\n",
    "        ax       = axes[i]\n",
    "    )\n",
    "    axes[i].set_title(colum, fontsize = 7, fontweight = \"bold\")\n",
    "    axes[i].tick_params(labelsize = 6)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    \n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top = 0.9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
