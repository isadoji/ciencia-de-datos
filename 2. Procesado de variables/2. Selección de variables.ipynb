{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipos de datos\n",
    "\n",
    "Es importante conocer con que tipos de datos estamos trabajando. Los datos pueden ser:\n",
    "\n",
    "1. Cuantitativos o numéricos (discretos y continuos)\n",
    "2. Cualitativos o categóricos\n",
    "3. Ordinales\n",
    "\n",
    "## Datos cuantitativos \n",
    "\n",
    "Son representados por números y son importantes si representan la medida de la cantidad observada \n",
    "de cierta característica. Pueden ser:\n",
    "\n",
    "1. Discretos: solo pueder asuamir un valor de una lista de números específicos, pueden ser contados y listados.\n",
    "\n",
    "2. Continuos: representan mediciones se suelen redondear a un número fijo de decimales para facilitar su manipulación.\n",
    "\n",
    "## Datos cualitativos\n",
    "\n",
    "Estos datos describen categorías no numéricas que representar determinada cualidad. Los datos categóricos pueden tomar valores numéricos (por ejemplo, \"0\" para indicar \"masculino\" y \"1\" para indicar \"femenino\"), pero esos números no tienen un sentido matemático.\n",
    "\n",
    "## Datos ordinales\n",
    "\n",
    " Es una categoría intermedia entre los cuantitativos y los cualitativos. En estos datos existe un orden (primero, segundo, tercero, etc.) es decir, podemos establecer un ranking, es decir, los datos son cualitativos, pero los números colocados en cada categoría tienen un significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"../datasets/\"\n",
    "filename = \"titanic-kaggle/train.csv\"\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "data = pd.read_csv (fullpath)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop(['Cabin','Name','Ticket'], axis=1)\n",
    "data1 = data1[data1['Embarked'].notna()]\n",
    "data1['Age'] = data1['Age'].fillna(-1)\n",
    "\n",
    "### Datos ordinales\n",
    "\n",
    "data1['Embarked'] = data1['Embarked'].map({'Q': 2, 'C': 1, 'S': 0})\n",
    "data1['Sex'] = data1['Sex'].map({'female': 1, 'male': 0})\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadistica basica de variables númericas \n",
    "data1.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Categorización de los datos en hombre/mujeres vivos/muertos\n",
    "\"\"\"\n",
    "live = data1[(data1[\"Survived\"]==1)]\n",
    "dead = data1[(data1[\"Survived\"]==0)]\n",
    "livem = data1[(data1[\"Survived\"]==1) & (data1[\"Sex\"] == 0)]\n",
    "livef = data1[(data1[\"Survived\"]==1) & (data1[\"Sex\"] == 1)]\n",
    "deadm = data1[(data1[\"Survived\"]==0) & (data1[\"Sex\"] == 0)]\n",
    "deadf = data1[(data1[\"Survived\"]==0) & (data1[\"Sex\"] == 1)]\n",
    "print(\"De los\",data1.shape[0], \"pasajeros del titanic\",\"sobrevivieron\",live.shape[0],\"y murieron\",dead.shape[0])\n",
    "print(\"De los\",live.shape[0], \"pasajeros que sobrevivieron\",livem.shape[0],\"eran hombres y\",livef.shape[0],\"eran mujeres\")\n",
    "print(\"De los\",dead.shape[0], \"pasajeros que murieron\",deadm.shape[0],\"eran hombres y\",deadf.shape[0],\"eran mujeres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico de barras de frecuencias relativas.\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,4,1)\n",
    "plot = (100 * data1['Survived'].value_counts() / len(data1['Survived'])).plot(\n",
    "kind='pie',  autopct='%.2f',title='Sobrevivientes %')\n",
    "plt.subplot(1,4,2)\n",
    "plot = (100 * data1['Pclass'].value_counts() / len(data1['Pclass'])).plot(\n",
    "kind='pie' , autopct='%.2f', title='Pasajeros del Titanic %')\n",
    "plt.subplot(1,4,3)\n",
    "plot = (100 * data1['Sex'].value_counts() / len(data1['Sex'])).plot(\n",
    "kind='pie' , autopct='%.2f', title='Sexo de los Pasajeros del Titanic %')\n",
    "#plt.subplot(1,4,4)\n",
    "#plot = (100 * data1['Age'].value_counts() / len(data1['Age'])).plot(\n",
    "#kind='pie' , autopct='%.2f', title='Edad de los Pasajeros del Titanic %')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de atributos\n",
    "\n",
    "Proceso para seleccionar un subconjunto de atributos (columnas) relevantes para la construcción del modelo predictivo. Se utiliza para identificar y eliminar los atributos innecesarios, irrelevantes y/o redundantes que no contribuyen al modelo predictivo o disminuyen su precisión.\n",
    "\n",
    "Para hacer la selección de atributos, hay que tener en cuenta la relación que existe entre ellos  y así poder eliminarlos de forma individual (univariante) o un un grupo de atributos en forma conjunta (multivariante).\n",
    "\n",
    "Ejemplo: \n",
    "\n",
    "* **Modelo bayesiano**: cada atributo es independiente, por lo tanto, podemos hacer una selección de variable univariante.\n",
    "* **Modelo de red neuronal**: no asume la independencia de los atributos (usa todos los disponibles) por lo tanto, podemos hacer una selección de variable multivariante.\n",
    "\n",
    "## Métodos para selección de atributos\n",
    "\n",
    "1. **Métodos de filtrado**\n",
    "\n",
    "Aplican una medida estadística para asignar una puntuación a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuación y son, o bien seleccionados para su conservación o eliminados del conjunto de datos. Los métodos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente.\n",
    "\n",
    "Ejemplos de estos métodos son: coeficientes de correlación de Pearson, prueba de $\\chi^2$, prueba de Fisher, ganancia de información.\n",
    "\n",
    "2. **Métodos empaquetados**\n",
    "\n",
    "Selección de un conjunto de atributos como un problema de búsqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuación a cada combinación basada en la precisión del modelo.\n",
    "\n",
    "Ejemplo de este método es el algoritmo de eliminación recursiva (forward, backward) de atributos.\n",
    "\n",
    "Bibliografía:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_selection\n",
    "\n",
    "https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coeficientes de correlación de Pearson\n",
    "### \n",
    "def train_corr(data): \n",
    "    correlation = data.corr()\n",
    "    sns.heatmap(correlation, annot=True, cbar=True, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable numérica\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
    "axes = axes.flat\n",
    "columnas = data1.select_dtypes(include=['float64', 'int']).columns\n",
    "columnas = columnas.drop('Survived') # objetivo\n",
    "\n",
    "for i, colum in enumerate(columnas):\n",
    "    sns.regplot(\n",
    "        x           = data1[colum],\n",
    "        y           = data1['Survived'],\n",
    "        color       = \"gray\",\n",
    "        marker      = '.',\n",
    "        scatter_kws = {\"alpha\":0.4},\n",
    "        line_kws    = {\"color\":\"r\",\"alpha\":0.7},\n",
    "        ax          = axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"Sobrevivientes vs {colum}\", fontsize = 7, fontweight = \"bold\")\n",
    "    axes[i].tick_params(labelsize = 6)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "# Se eliminan los axes vacíos\n",
    "for i in [8]:\n",
    "    fig.delaxes(axes[i])\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Correlación con Sobrevivientes', fontsize = 10, fontweight = \"bold\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn feature selection\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest #SelectKBest removes all but the highest scoring features\n",
    "from sklearn.feature_selection import chi2, f_classif #Fisher\n",
    "from sklearn.feature_selection import RFE # recursive feature elimination\n",
    "from sklearn.ensemble import ExtraTreesClassifier #decision trees "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separamos las columnas objetivo\n",
    "\n",
    "* Objetivo: y\n",
    "* Atributos: x\n",
    "\n",
    "En este caso queremos saber quien sobrevivio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo va a aplicar la prueba de Fisher a todos los atributos y va a seleccionar los que mejor resultado obtuvieron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data2['Survived']\n",
    "k = 4  # número de atributos a seleccionar\n",
    "entrenar = data2.drop(['Survived'], axis=1)\n",
    "columnas = list(entrenar.columns.values)\n",
    "seleccionadas = SelectKBest(f_classif, k=k).fit(entrenar, x)\n",
    "atrib = seleccionadas.get_support()\n",
    "atributos = [columnas[i] for i in list(atrib.nonzero()[0])]\n",
    "atributos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos empaquetados: Eliminación recursiva\n",
    "\n",
    "Selecciona recursivamente un número cada vez más pequeño de atributos. Primero comienza con todos los atributos del dataset y luego en cada pasada va eliminando aquellos que tenga el menor coeficiente de importancia hasta alcanzar el número de atributos deseado\n",
    "\n",
    "Es más precisa, pero consume mucho más recursos, ya que requiere entrenar un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ExtraTreesClassifier() \n",
    "erec = RFE(modelo)  \n",
    "erec = erec.fit(entrenar, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrib = erec.support_\n",
    "atributos = [columnas[i] for i in list(atrib.nonzero()[0])]\n",
    "atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de atributos.\n",
    "modelo.fit(entrenar, x)\n",
    "modelo.feature_importances_[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(modelo.feature_importances_)[::-1][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de componentes principales\n",
    "\n",
    "Transformación del conjunto original de $m$ variables en otro conjunto de nuevas de $p$ variables ($p<m$) que no esten correlacionadas. \n",
    "\n",
    "Conseguir $p$ variables ortogonales nuevas (componentes principales) y cada una de ellas es una combinación lineal de las $m$ variables originales, de forma que expliquen la mayor cantidad de variabilidad del conjunto de datos original posible. \n",
    "\n",
    "## Valor y vector propios\n",
    "\n",
    "Sea $A$ como una matriz de $m\\times m$ y $v$ un vector de dimensión $1\\times m$ no nulo,entonce , $v$ es un vector propio de $A$ si existe un escalar $c$ tal que:\n",
    "$\n",
    "\\begin{align}\n",
    "A v = c v\n",
    "\\end{align}\n",
    "\n",
    "donde $c$ es el valor propio asociado al vector propio $v$.\n",
    "\n",
    "Los valores propios de la matriz $A$ se pueden obtener con los polinomios característicos, entonces si:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "det (A c - I) = 0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "los ceros de este polinomio son los valores propios de la matriz $A$.\n",
    "\n",
    "Para el análisis de componentes principales, cada componente principal es un vector propio de la matriz de covarianzas y la varianza explicada de los datos originales por cada componente principal es el valor propio asociado al vector propio en cuestión. \n",
    "\n",
    "Como estamos tratando de recoger la mayor varianza posible, las variables que tengan una magnitud mayor que otras tendrán mayor escala de varianza y, por tanto, dominarían sobre las otras variables, por lo que primero hay que normalizar las variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalamiento de las características\n",
    "\n",
    "Consiste en transformar los datos de entrada (columnas) a una escala común para asegurarse de que cada variable de entrada contribuya de manera equitativa al rendimiento del modelo. \n",
    "\n",
    "1. **Escalamiento** (normalización): escala las características de entrada a un rango entre 0 y 1 restando el valor mínimo de la característica a cada punto de datos y dividiendo el resultado por el rango de la característica. Es especialmente útil para datos con distribuciones sesgadas.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "X'= \\frac{X - X_{min}}{X_{max} -X_{min}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "2. **Estandarización**: transforma los datos de entrada de manera que tengan una media de 0 y una desviación estándar de 1, restando la media de la característica a cada punto de datos y dividiendo el resultado por la desviación estándar. La estandarización es beneficiosa cuando se trabaja con algoritmos que asumen que las características de entrada siguen una distribución gaussiana.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "X'= \\frac{X - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.iloc[:, 2:].values # caracteristicas\n",
    "Y = data1.iloc[:, 1].values # variable a predecir\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)#fits the parameters of the data and then transforms it.\n",
    "#X_test = sc.transform(X)#transforms the data by using parameters already stored in the class\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCA = PCA(n_components=2)\n",
    "components = PCA.fit_transform(X)\n",
    "PCA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumVar = pd.DataFrame(np.cumsum(PCA.explained_variance_ratio_)*100, \n",
    "                      columns=[\"cumVarPerc\"])\n",
    "expVar = pd.DataFrame(PCA.explained_variance_ratio_*100, columns=[\"VarPerc\"])\n",
    "pd.concat([expVar, cumVar], axis=1)\\\n",
    "    .rename(index={0: \"PC1\", 1: \"PC2\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
