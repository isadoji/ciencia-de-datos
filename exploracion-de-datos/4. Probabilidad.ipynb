{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Función de probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables aleatorias \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Un experimento estadístico es cualquier proceso que proporciona datos. \n",
    "2. Estos datos tienen que convertirse en descripciones numéricas del resultado.\n",
    "3. Estas descripciones numéricas son observaciones aleatorias. \n",
    "4. A las observaciones aleatorias se les considera como la expresión en cada caso concreto de una variable aleatoria que toma valores en los resultados del experimento.\n",
    "\n",
    "**Variable aleatoria** es una variable matemática cuyos posibles valores son las descripciones numéricas de todos los resultados posibles de un experimento\n",
    "estadístico.\n",
    "\n",
    "Se pueden distinguir distintos tipos de variables aleatorias:\n",
    "\n",
    "1. _Variables cuantitativas_: son las que resultan de experimentos cuyos resultados son directamente numéricos.\n",
    "\n",
    "2. _Variables cualitativas_: son las que proceden de experimentos cuyos resultados expresan una cualidad no numérica que necesita ser cuantificada.\n",
    "\n",
    "3. _Variables discretas_: son aquellas que se define sobre un espacio muestral numerable, finito o infinito. \n",
    "\n",
    "4. _Variables continuas_: son aquellas que se definen sobre un espacio asimilable al conjunto de los números reales, es decir, un espacio no numerable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables aleatorias discretas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una variable aleatoria discreta toma cada uno de sus valores con una determinada probabilidad.\n",
    "\n",
    "**Función de probabilidad**: es una función tal que, al sustituir x por un valor de la variable, el valor que toma la función es la probabilidad de que la variable X asuma el valor x.\n",
    "\n",
    "La función de probabilidad se representa como: \n",
    "\n",
    "$f(x)=P(X=x)$\n",
    "\n",
    "Las funciones de probabilidad sólo se definen para los valores de la variable aleatoria y deben cumplir tres propiedades:\n",
    "\n",
    "1. $\\forall  x \\in R, f(x) \\geq 0$\n",
    "2. $\\sum_{x} f(x) = 1$\n",
    "3. $P(X=x)=f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de distribución** La función de distribución se define para todos los números reales, no sólo para los valores de la variable. Su máximo es siempre 1 pues cuando el valor que se sustituye es mayor o igual que el valor máximo de la variable, la probabilidad de que ésta tome valores menores o iguales que el sustituido es la\n",
    "probabilidad del espacio muestral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables aleatorias continuas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de densidad** Una variable aleatoria continua tiene la característica de tomar cada uno de sus valores con\n",
    "probabilidad infinitesimal. \n",
    "\n",
    "Puede calcularse la probabilidad de que la variable tome valores en determinados intervalos.\n",
    "\n",
    "$P(a \\leq X \\leq b) = P(X = a) + P(a < X < b) + P(X = b) = P(a < X < b)$\n",
    "\n",
    "La función de densidad debe cumplir tres condiciones análogas a las de la función de probabilidad:\n",
    "\n",
    "1. $\\forall  x \\in R f(x) \\geq  0$\n",
    "2. $\\int_{-\\infty}^{\\infty} f(x) dx = 1$\n",
    "3. $P(a \\leq X \\leq b)=\\int_{a}^{b}f(x) dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de densidad** es una función continua tal que su integral entre los extremos de un intervalo nos da el valor de la probabilidad de que X tome valores en ese intervalo.\n",
    "\n",
    "$P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución conjunta de dos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos dos variables aleatorias X y Y, si queremos estudiarlas conjuntamente debemos establecer una relación que ligue los valores de una con los de la otra.\n",
    "\n",
    "Para variables discretas, se puede establecer una función de probabilidad para las posibles parejas de valores de ambas variables; a esta función se le llama **función de probabilidad conjunta, f(x,y).**\n",
    "\n",
    "$P[(X=x)\\cap(Y=y)]= f(x,y)$\n",
    "\n",
    "Que puede ser discreta o continua y que cumple las condiciones de la función de probabilida o función de densiadad\n",
    "\n",
    "\n",
    "1. $\\forall  x,y \\in R f(x,y) \\geq  0$\n",
    "2. $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1$\n",
    "3. $P[(x,y)\\in A]=\\int\\int_A f(x,y) dx dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor esperado de una variable\n",
    "\n",
    "**Variables aleatorias independientes**: dos variables aleatorias X y Y, discretas o continuas cuyas funciones de probabilidad o densidad son $g(x)$ y $h(y)$, respectivamente, con función de probabilidad o densidad conjunta $f(x,y)$, son estadísticamente independientes si y sólo si $f(x,y)=g(x)\\cdot f(x)$\n",
    "\n",
    "Supongamos que hemos realizado n veces un experimento aleatorio que genera una variable X. El valor medio del experimento en estas n repeticiones es la suma de los productos de los valores de la variable por su frecuencia relativa. Cuando n sea igual a infinito, el valor medio del experimento se llama valor esperado o esperanza matemática:\n",
    "\n",
    "$\\mathbb{E}[X]=x_1p(X=x_1)+\\ldots+x_ip(X=x_i)=\\sum_i^nx_ip(x_i)$\n",
    "\n",
    "Para una variable aleatoria continua, la esperanza se calcula mediante la integral de todos los valores y la función de densidad:\n",
    "\n",
    "$\\mathbb{E}[X]=\\int_{-\\infty}^\\infty x f(x) dx$.\n",
    "\n",
    "La esperanza también se suele simbolizar con $\\mathbb{E}[X]=\\mu$\n",
    "\n",
    "**Propiedades**\n",
    "\n",
    "1. Si X es siempre positiva, entonces siempre lo es $\\mathbb{E}[X]$.\n",
    "2. La esperanza matemática de una constante es igual a esa misma constante, es decir, $\\mathbb {E}[c]=c$.\n",
    "3. Si X está delimitada por dos números reales, a y b, tal que: $a \\leq X \\leq b$, entonces también lo está su media: $a \\leq \\mathbb{E}[X] \\leq b$\n",
    "4. Linealidad. Si existe $\\mathbb{E}[X]$ y se considera $Y=a+bX$, entonces $\\mathbb{E}[Y]=\\mathbb{E}[a+bX]=a+b\\mathbb{E}[X]$\n",
    "\n",
    "además la esperanza es un operador **lineal**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentos de una variable\n",
    "\n",
    "Dada una variable aleatoria X con función de probabilidad o densidad f(x) podemos definir una función de X que sea igual a la diferencia entre la variable y su media elevada a un exponente entero no negativo:\n",
    "\n",
    "$z(x)=(x-\\mu)^k$ siendo  $k\\in Z, k\\geq 0$\n",
    "\n",
    "El valor esperado de $z(x)$ es el k-ésimo momento de la variable X respecto a su origen:\n",
    "\n",
    "$\\mu_k=\\mathbb{E}[(x-\\mu )^k]= \\left \\{ \\begin{matrix} \\sum_x (x-\\mu)^k f(x) & si~ X~ es discreta \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^k f(x) dx & si~ X~ es continua\\end{matrix}\\right.$\n",
    "\n",
    "$k=0, \\mu_0=1$;\n",
    "\n",
    "$k=1, \\mu_1=\\mathbb{E}[(x-\\mu)^1]=\n",
    "\\mathbb{E}[(x-\\mu)]=\\mathbb{E}[X]-\\mu=0$\n",
    "\n",
    "$k=2, \\mu_2=\\mathbb{E}[(x-\\mu )^2]=\\sigma^2$ \n",
    "\n",
    "### Varianza\n",
    "\n",
    "La varianza de una variable mide la dispersión de sus valores respecto al valor central $\\mu$\n",
    "\n",
    "$\\mu_2=\\mathbb{E}[(x-\\mu )^2]= \\mathbb{E}[X^{2}]-\\mu^{2}=\\mathbb{E}[X^{2}]-\\mathbb{E}[X]^{2}= \\left \\{ \\begin{matrix} \\sum_x (x-\\mu)^2 f(x) & si~ X~ es~ discreta \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) dx & si~ X~ es~ continua\\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "Es decir, la varianza de una variable es igual a la media de los cuadrados menos el cuadrado\n",
    "de la media.\n",
    "\n",
    "La varianza se expresa en unidades cuadráticas que no siempre tienen una interpretación clara. \n",
    "\n",
    "**Desviación estandar $\\sigma$:** Medida de la\n",
    "dispersión que que se calcula como la raíz cuadrada positiva de la varianza. La desviación estandar se mide en las mismas unidades que la\n",
    "variable\n",
    "\n",
    "$\\sigma_x =+\\sqrt{\\sigma^2_x}$\n",
    "\n",
    "La desviación estandar no resuelve todos los problemas que se pueden plantear, por ejemplo la comparación de situaciones en las que la unidad de medida sea diferente. Para resolver esta cuestión se define una medida adimensional de\n",
    "la variabilidad que es el coeficiente de variación, $C_V$, que se calcula como el cociente entre la desviación típica y la media.\n",
    "\n",
    "$C_V=\\frac{\\sigma}{\\mu}$\n",
    "\n",
    "o porcentual\n",
    "\n",
    "$C_V=100 \\cdot\\frac{\\sigma}{\\mu}$\n",
    "\n",
    "### Varianza de variables asociadas\n",
    "\n",
    "Supongamos que tenemos dos variables aleatorias X y Y, discretas o continuas, con función\n",
    "de probabilidad o densidad conjunta f(x,y) y definimos una función z(x,y):\n",
    "\n",
    "$z(x,y)=(x-\\mu_x)(y-\\mu_y)$\n",
    "\n",
    "Al valor esperado de z(x,y) se le llama **covarianza $\\sigma_{xy}$** o **cov(x,y)** de las variables X y Y.\n",
    "\n",
    "$\\mu_{xy}=\\mathbb{E}[(x-\\mu_{x})][(y-\\mu_{y})]=$\n",
    "$\\left\\{\\begin{matrix} \n",
    "\\sum_x\\sum_y (x-\\mu_x)(y-\\mu_y) f(x,y) &\n",
    "si~ X~ y~ Y~ son~ discreta \\\\ \n",
    "\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} (x-\\mu_x)(y-\\mu_y) f(x,y) dx & si~ X~ y~ Y~ son~ continua\\end{matrix}\\right.$\n",
    "\n",
    "La covarianza es una medida de la variación común a dos variables y, por tanto, una medida\n",
    "del grado y tipo de su relación.\n",
    "1. $\\sigma_{xy}$ es positiva si los valores más grandes de X están asociados a los valores más grandes de Y y viceversa.\n",
    "2. $\\sigma_{xy}$ es negativa si los más bajos de X están asociados a los valores más bajos de Y y\n",
    "viceversa.\n",
    "3. Si X y Y son variables aleatorias independientes cov(x,y) = 0 (la independencia es condición suficiente pero no necesaria para que la cov(x,y) sea nula.)\n",
    "\n",
    "La covarianza de dos variables se puede calcular como:\n",
    "\n",
    "$\\sigma_{xy}=\\mathbb{E}[(x-\\mu_x)(y-\\mu_y)]=\\mathbb{E}[X\\cdot Y]-\\mu_x \\mu_y=\\mathbb{E}[X\\cdot Y]-\\mathbb{E}[X]\\cdot\\mathbb{E}[Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La covarianza se expresa en términos del producto de las unidades de medida de\n",
    "ambas variables, lo cual no siempre es fácilmente interpretable. Por otra parte también es difícil comparar situaciones diferentes entre sí. Ambos problemas se solucionan mediante la definición del coeficiente de correlación.\n",
    "\n",
    "**Coeficiente de correlación $\\rho$:** Es el cociente entre la covarianza y el producto de las desviaciones estandar de las dos variables.\n",
    "\n",
    "$\\rho=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$\n",
    "\n",
    "1. La correlación toma valores entre -1 y 1, siendo su signo igual al de la covarianza.\n",
    "2. Correlaciones con valor absoluto 1 implican que existe una asociación matemática lineal perfecta, positiva o negativa, entre las dos variables.\n",
    "3. Correlaciones iguales a 0 implican ausencia de asociación (las variables independientes tienen correlación 0, pero la independencia es condición suficiente pero no necesaria.)\n",
    "4. Correlaciones con valores absolutos intermedios indican cierto grado de asociación entre los valores de las variables.\n",
    "\n",
    "$k=2, \\mu_2=\\mathbb{E}[(x-\\mu )^2]=\\sigma^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asimetría\n",
    "\n",
    "El tercer momento respecto de la media, mide la asimetría de la distribución, es decir, si\n",
    "existen o no observaciones muy extremas en algún sentido con frecuencias razonablemente altas.\n",
    "\n",
    "$k=3, \\mu_3=\\mathbb{E}[(x-\\mu)^3]$ \n",
    "\n",
    "\n",
    "1. Si la asimetría es negativa, la variable toma valores muy bajos con mayor frecuencia que valores muy altos y (asimétrica hacia la izquierda). \n",
    "\n",
    "2. Si la asimetría es positiva, la variable toma valores muy altos con mayor frecuencia que valores muy bajos (asimétrica hacia la derecha). \n",
    "\n",
    "3. Si la asimetría es cero, los valores bajos y altos de la variable tienen probabilidades iguales (por ejemplo la distribución normal)\n",
    "\n",
    "La asimetría tiene el mismo problema que la varianza y la covarianza en cuanto a sus\n",
    "unidades de medida y, por ello, normalmente se utiliza una medida adimensional de la asimetría\n",
    "que es el coeficiente de asimetría, $g_1$ , que se calcula como el cociente entre el tercer momento y el cubo de la desviación estandar.\n",
    "\n",
    "$g_1=\\frac{\\mu_3}{\\sigma_3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curtosis\n",
    "\n",
    "El cuarto momento respecto de la media, mide la forma de la distribución de probabilidad o **curtosis**.\n",
    "\n",
    "$k=4, \\mu_4=\\mathbb{E}[(x-\\mu)^4]$ \n",
    "\n",
    "1. Curtosis pequeña (platicúrticas): curvas o histogramas con colas cortas y aspecto aplanado o en meseta.\n",
    "\n",
    "2. Curtosis grande (leptocúrtica): gráfica alta y estilizada, con colas largas y pesadas.\n",
    "\n",
    "La curtosis de una variable siempre es positiva y se mide en la unidades de la variable\n",
    "elevadas a potencia 4. Por tanto, tiene los mismos problemas relacionados con las\n",
    "unidades de medida y las escalas que los momentos menores.\n",
    "\n",
    "**Coefeiciente de curtosis** $g_2$: Medida adimensional de la curtosis, se calcula como el\n",
    "cociente entre el cuarto momento y el cuadrado de la varianza, al que se le resta 3 unidades. Esta corrección se debe a que, sin ella, las variables normales tendrían coeficiente de curtosis igual a 3; al restar 3 conseguimos que el coeficiente de curtosis de la variable normal sea 0 y que las variables platicúrticas tengan coeficiente de curtosis negativo y la leptocúrticas positivo.\n",
    "\n",
    "$g_2=\\frac{\\mu_4}{\\sigma_4}-3$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
