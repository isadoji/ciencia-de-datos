{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer datos con la función **read_csv**\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/customer-churn-model/Customer Churn Model.csv\") #filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros de la función **read_csv** (30 argumentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de los parámetros de la función read_csv\n",
    "```\n",
    "read.csv(filepath=\"../datasets/customer-churn-model/Customer Churn Model.csv/titanic3.csv\",\n",
    "        sep = \",\",         \n",
    "        dtype={\"ingresos\":np.float64, \"edad\":np.int32}, \n",
    "        header=0,names={\"ingresos\", \"edad\"},\n",
    "        skiprows=12, \n",
    "        index_col=None, \n",
    "        skip_blank_lines=False, \n",
    "        na_filter=False\n",
    "        )\n",
    "```\n",
    "1. **sep** separado por comas\n",
    "2. **dtype** columna ingresos de tipo flotante. El valor determinado es None, quiere decir que panda asignara el tipo que mas le convenga\n",
    "3. **header** donde esta la cabecera, y cuales quieres utilizar\n",
    "4. **skiperows** saltar filas \n",
    "5. **index_col** cambiar la columna del index\n",
    "6. **skipe_blank** saltar lineas en blancos\n",
    "7. **na_filter** elimina NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"../datasets/\" #carpeta global\n",
    "filename = \"customer-churn-model/Customer Churn Model.csv\" #dataset\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "data = pd.read_csv (fullpath)\n",
    "data2 = pd.read_csv(mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\")\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiar la cabecera a data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer un archivo con cabecera\n",
    "data_cols = pd.read_csv(mainpath + \"/\" + \"customer-churn-model/Customer Churn Columns.csv\")\n",
    "# Convertirlo a una lista\n",
    "data_col_list = data_cols[\"Column_Names\"].tolist()\n",
    "# Quitar la cabecera original (header=None) y añadir la lista como cabecra (names=data_col_list)\n",
    "data2 = pd.read_csv(mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\", header=None, names=data_col_list)\n",
    "#data2.columns.values\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos a través de la función **open**\n",
    "\n",
    "Leer el archivo de datos fila por fila en un loop, almacenar parte del conjunto de datos y cerrar el archivo\n",
    "\n",
    "1. **r** solo lectura (\"a\" (append), posicionará el cursor al final y no borrará el contenido del mismo.)\n",
    "2. **strip** se utiliza para eliminar los espacios en blanco al inicio y al final de la linea\n",
    "3. **split** divide la linea de texto\n",
    "4. **len(cols) es el número de columnas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = open(mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\",'r')\n",
    "cols = data3.readline().strip().split(\",\")\n",
    "n_cols = len(cols)\n",
    "counter = 0 #inicializar contador\n",
    "main_dict = {} #definir diccionario con nombre de las columnas\n",
    "for col in cols:\n",
    "    main_dict[col] = [] #vacio\n",
    "main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data3: # interación sobre las filas de los datos\n",
    "    values = line.strip().split(\",\") #quita espacios sobrantes y divide por comas\n",
    "    for i in range(n_cols): #intera sobre las columnas\n",
    "        main_dict[cols[i]].append(values[i]) #agrega las filas a cada columna \n",
    "    counter += 1 #cuenta cuantas filas hay\n",
    "   \n",
    "print(\"El conjunto de datos tiene %d filas y %d columnas\"%(counter-1 , n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(main_dict) #crear un dat frame\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura archivos: tabuladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = mainpath + \"/\" + \"customer-churn-model/Customer Churn Model.txt\"\n",
    "outfile = mainpath + \"/\" + \"customer-churn-model/Table Customer Churn Model.txt\"\n",
    "with open(infile, \"r\") as infile1:\n",
    "    with open(outfile, \"w\") as outfile1:\n",
    "        for line in infile1:\n",
    "            fields = line.strip().split(\",\")\n",
    "            outfile1.write(\"\\t\".join(fields)) #tabulador\n",
    "            outfile1.write(\"\\n\") #salto de linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(outfile, sep = \"\\t\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer datos desde una URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medals_url = \"http://winterolympicsmedals.com/medals.csv\"\n",
    "medals_data = pd.read_csv(medals_url)\n",
    "medals_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando las bibliotecas de python **csv** y **urllib3**\n",
    "\n",
    "https://docs.python.org/3/library/csv.html\n",
    "\n",
    "https://urllib3.readthedocs.io/en/stable/\n",
    "\n",
    "$ conda install -c conda-forge urllib3\n",
    "\n",
    "\n",
    "Usando la librería urllib3 para leer los datos desde una URL externa, procesarlos y convertirlos a un data frame de *python* antes de guardarlos en un CSV local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', medals_url)\n",
    "r.status\n",
    "response = r.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = csv.reader(response)\n",
    "#El objeto reponse contiene un string binario, así que lo convertimos a un string descodificándolo en UTF-8\n",
    "str_data = response.decode(\"utf-8\")\n",
    "#Dividimos el string en un array de filas, separándolo por intros\n",
    "lines = str_data.split(\"\\n\")\n",
    "#La primera línea contiene la cabecera, así que la extraemos\n",
    "col_names = lines[0].split(\",\")\n",
    "n_cols = len(col_names)\n",
    "counter = 0 #inicializar contador\n",
    "main_dict = {} #definir diccionario con nombre de las columnas\n",
    "for col in col_names:\n",
    "    main_dict[col] = [] #vacio\n",
    "main_dict\n",
    "for line in lines:\n",
    "    if(counter > 0):\n",
    "        values = line.strip().split(\",\")\n",
    "        for i in range(len(col_names)):\n",
    "            main_dict[col_names[i]].append(values[i]) #agrega las filas a cada columna \n",
    "    counter += 1 #cuenta cuantas filas hay\n",
    "   \n",
    "print(\"El conjunto de datos tiene %d filas y %d columnas\"%(counter-1, n_cols))\n",
    "df5 = pd.DataFrame(main_dict) #crear un dat frame\n",
    "#df5.head()\n",
    "#Elegimos donde guardarlo (en la carpeta athletes es donde tiene más sentido por el contexto del análisis)\n",
    "mainpath = \"../datasets/\" #carpeta global\n",
    "filename = \"athlets/athlets\" #dataset\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "\n",
    "#Lo guardamos en CSV, en JSON o en Excel según queramos\n",
    "df5.to_csv(fullpath+\".csv\")\n",
    "df5.to_json(fullpath+\".json\")\n",
    "#df5.to_excel(fullpath+\".xls\")\n",
    "print(\"Los ficheros se han guardado correctamente en: \"+fullpath)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivos XLS y XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"athlets/athlets.xls\" #dataset\n",
    "fullpath = os.path.join(mainpath, filename)\n",
    "data = pd.read_excel(fullpath)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "819112c24f0d6b36d35f6c5653e120a0c93a25f82bf2809eaf9b65613f02e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
